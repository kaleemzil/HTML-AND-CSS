<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI AND CHATBOT</title>
    <link rel="stylesheet" href="../css/style.css">
</head>

<body>
    <button><a href="../INDEX.HTML">BACK</a></button>
    <H1>AI AND CHATBOT</H1>
    <p>Law enforcement and criminal justice authorities are increasingly using artificial intelligence (AI) and
        automated decision-making (ADM) systems. These systems are often used to profile people, ‘predict’ their
        actions, and assess their risk of certain behaviour, such as committing a crime, in the future. This can have
        devastating consequences for the people involved, who are profiled as criminals or considered a risk even though
        they haven’t actually committed a crime.</p>

    <p>Predictive policing is no longer confined to the realms of science fiction. These systems are being used by law
        enforcement around the world. And predictions, profiles, and risk assessments that are based on data analysis,
        algorithms and AI, often lead to real criminal justice outcomes. These can include constant surveillance,
        repeated stop and searches, questioning, fines and arrests. These systems can also heavily influence sentencing,
        prosecution and probation decisions.</p>
    <H1>Predictive Policing</H1>
    <P>Law enforcement agencies are increasingly using artificial intelligence, algorithms and big data to profile
        people and ‘predict’ whether they are likely to commit a crime. Predictive policing has been proven time and
        time again to reinforce discrimination and undermine fundamental rights, including the right to a fair trial and
        the presumption of innocence. This results in Black people, Roma, and other minoritised ethnic people being
        overpoliced and disproportionately detained and imprisoned across Europe.</P>
    <P>For example, Delia, a predictive system in Italy, uses ethnicity data to profile and ‘predict’ future
        criminality. In the Netherlands, the Top 600 list attempts to ‘predict’ which young people will commit
        high-impact crime. One in three of the ‘Top 600’ – many of whom have reported being followed and harassed by
        police – are of Moroccan descent.

        Only an outright ban can stop this injustice. We have been campaigning for a prohibition in the EU’s Artificial
        Intelligence Act (AI Act). This ban must cover predictive policing of both individuals and places, as both
        methods are equally harmful.</P>
    <H1>What are the problems with AI?</H1>
    <P>There are fundamental flaws in how AI and automated systems are being implemented in criminal justice:

        Discrimination and bias: AI and automated systems in criminal justice are designed, created and operated in a
        way that makes them predisposed to produce biased outcomes. This can stem from their purpose, such as targeting
        a certain type of crime or a specific area. It can also be a result of their use of biased data, which reflects
        structural inequalities in society and institutional biases in criminal justice and policing. As a result, these
        systems can reproduce and exacerbate discrimination based on race, ethnicity, nationality, socio-economic status
        and other grounds. These systemic, institutional and societal biases are so ingrained that it is questionable
        whether any AI or ADM system would produce unbiased outcomes.

        Infringement of the presumption of innocence: Profiling people and taking action before a crime has been
        committed undermines the right to be presumed innocent until proven guilty in criminal proceedings. Often, these
        profiles and decisions are based not just on an individual’s behaviour but on factors far beyond their control.
        This may include the actions of people they are in contact with or even demographic information, such as data
        about the neighbourhood they live in.

        Lack of transparency and routes for redress: Any system that has an influence on criminal justice decisions
        should be open to public scrutiny. However, technological barriers and deliberate efforts to conceal how the
        systems work for profit-driven reasons make it difficult to understand how such decisions are made. People are
        often unaware that they have been subject to an automated decision. Clear routes for challenging decisions – or
        the systems themselves – and for redress are also severely lacking.

        These are serious issues that can seriously impact people’s lives, threaten equality and infringe fundamental
        rights, including the right to a fair trial.</P>
</body>

</html>